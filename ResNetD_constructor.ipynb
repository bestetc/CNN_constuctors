{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-D Constructor\n",
    "### В пайплайне используется контруктор ResNetD сетей и некоторые трюки, взятые из [статьи](https://arxiv.org/pdf/1812.01187.pdf) Bag of Tricks\n",
    "#### Загрузка и просмотр датасета лежат в [этом ноутбуке](ResNet_constructor.ipynb) \n",
    "- Accuracy на тестовом датасете составляет ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 32\n",
    "num_epoch = 255\n",
    "optimizer_type = 'SGD' # 'SGD' or 'Adam'\n",
    "label_smoothing = 0.05 # 0 for disable label_smoothing\n",
    "# neural network architecture\n",
    "resnet_layers = [3,4,6,3]\n",
    "bottleneck = False\n",
    "num_classes = 10\n",
    "# learning rate\n",
    "learning_rate = 0.1 # 0.1 * batch_size / 256\n",
    "warmup_epoch = 5\n",
    "sheduler_type = 'cos' # 'cosine' or 'step'\n",
    "sheduler_cycle = 2\n",
    "# misc\n",
    "save_best_model = True\n",
    "save_model_dir = './models/'\n",
    "save_metrics_dir = './metrics/'\n",
    "dataset_path = '../imagenette/imagenette2-320/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры_обучения\n",
    "- batch_size: кол-во изображений в одном батче. Предел зависит от кол-ва памяти на видеокарте\n",
    "- num_epoch: кол-во эпох обучения. Желательно добавлять к плановому количеству warmup_epoch\n",
    "- optimizer_type: тип оптимизатора, использующегося для обновления весов сети. Может быть 'SGD' или 'Adam'\n",
    "- label_smoothing: параметр сдвига целевой вероятности (epsilon). Подробности в статье Bag of Tricks\n",
    "\n",
    "<b>Архитектура сети:\n",
    " - layers: список с количеством стандартных блоков по слоям\n",
    " - bottleneck: определяет использование стандартных блоков или 'bottleneck' блоков \n",
    " - num_classes: количество предсказываемых классов<br>\n",
    "(!) Сеть ожидает на вход изображение с разрешением 224х224х3\n",
    "\n",
    "<b>Изменения скорости обучения\n",
    " - learning_rate: базовая скорость обучения\n",
    " - warmup_epoch: количество эпох, в течении которых происходит увеличение скорости обучения с 0 до базового значения\n",
    " - sheduler_type: задает стратегию изменения скорости обучения в течении обучения. может быть 'cos' или 'step'\n",
    "    - 'cos': скорость обучения убывает согласно функции косинуса до нуля. В конце цикла скорость обучения возвращается к базовому значению\n",
    "    - 'step': скорость обучения убывает ступенчато, снижаясь в 10 раз. Количество снижений указывается в sheduler_cycle\n",
    " - sheduler_cycle: задает кол-во циклов изменения learning rate. Должно быть меньше или равно num_epoch<br> \n",
    "Для 'cos' интерпритируется как кол-во циклов убывания learning rate с возвратом к стартовому learning rate в начале нового цикла<br>\n",
    "Для 'step' интерпритируется как кол-во уменьшений learning rate\n",
    "\n",
    "<b>Прочее\n",
    "- save_best_model: нужно ли сохранять лучшую модель в процессе обучения. False приведет к сохранению модели в конце обучения\n",
    "- save_model_dir: путь к папке, в которую сохраняются модели\n",
    "- save_metrics_dir: путь к папке, в которую сохраняются метрики обучения\n",
    "- dataset_path: путь к папке с датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры стандартных сетей:<br>\n",
    "<b>ResNet-18: </b> <br> \n",
    "model = ResNet_like(layers=[2,2,2,2], bottleneck=False, num_classes=10)\n",
    "\n",
    "<b>ResNet-36: </b> <br> \n",
    "model = ResNet_like(resnet_layers=[3,4,6,3], bottleneck=False, num_classes=10)\n",
    "\n",
    "<b>ResNet-50: </b> <br> \n",
    "model = ResNet_like(resnet_layers=[3,4,6,3], bottleneck=True, num_classes=10)\n",
    "\n",
    "<b>ResNet-101: </b> <br> \n",
    "model = ResNet_like(resnet_layers=[3,4,23,3], bottleneck=True, num_classes=10)\n",
    "\n",
    "<b>ResNet-152: </b> <br> \n",
    "model = ResNet_like(resnet_layers=[3,8,36,3], bottleneck=True, num_classes=10)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Название модели</b><br>\n",
    "Создаем название модели, которое будет фигурировать в названии сохраненных файлов метрики и модели<br>\n",
    "Название модели является производным от гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet34_SGD_lr0.1_b32_cos_sc125'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if bottleneck == True:\n",
    "    model_name = f'ResNet{sum(resnet_layers)*3+2}_{optimizer_type}_lr{learning_rate}_b{batch_size}_{sheduler_type}_sc{(num_epoch-warmup_epoch)//sheduler_cycle}'\n",
    "elif bottleneck == False:\n",
    "    model_name = f'ResNet{sum(resnet_layers)*2+2}_{optimizer_type}_lr{learning_rate}_b{batch_size}_{sheduler_type}_sc{(num_epoch-warmup_epoch)//sheduler_cycle}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем количество изображения в тренировочном и тестовом датасетах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9469, 3925)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(dataset_path + 'noisy_imagenette.csv')\n",
    "train_img_qty = len(labels_df[labels_df['is_valid'] == False])\n",
    "val_img_qty = len(labels_df[labels_df['is_valid'] == True])\n",
    "train_img_qty, val_img_qty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем DataLoader попутно предобрабатывая данные\n",
    "- Загрузку датасета можно найти в [ResNet_constructor.ipynb](./ResNet_constructor.ipynb)\n",
    "- Предварительный просмотр данных можно найти в [ResNet_constructor.ipynb](./ResNet_constructor.ipynb)\n",
    "\n",
    "В качетсве аугментаций ипользуется:\n",
    "- уменьшение картинки до разрешения 260*260\n",
    "- вырезка случайного квадрата размером 224*244 (сеть ожидает именно эту размерность)\n",
    "- переворот изображения по горизонтальной оси\n",
    "- нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "#         transforms.Resize((260,260)),\n",
    "#         transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "#         transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "trainset = datasets.ImageFolder(root=dataset_path + 'train/', transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "testset = datasets.ImageFolder(root=dataset_path+'val/', transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, #batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем количество батчей в trainloader'e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_per_epoch = len(trainloader)\n",
    "batch_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конструктор ResNet-like сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(!) Конструктор ожидает на вход изображение с разрешением 224х224х3<br>\n",
    "Выносим в функции сверточные слои для уменьшения количества букв в коде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1,padding=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем базовые блоки через классы.\n",
    "Класс NormalBlock собирает стандартный ResNet блок с skipconnection'ом\n",
    "Класс BottleneckBlock собирает Bottleneck ResNet блок с skipconnection'ом\n",
    "\n",
    "Каждый класс ожидает параметры:\n",
    " - num_layer - порядковый номер слоя, в котором будет использоваться данных блок. В стандартной ResNet архитектуре блоки используются со второго слоя.\n",
    " - downsample - определяет тип downsampling'а.\n",
    "     - 0 - downsampling не используется\n",
    "     - 1 - downsampling используется в блоке, где уменьшается разрешение и увеличивается кол-во каналов\n",
    "     - -1 - downsampling используется в блоке, где разрешение не уменьшается, но увеличивается кол-во каналов (обычно последний слой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layer,\n",
    "        downsample = 0,\n",
    "    ):\n",
    "        super(NormalBlock, self).__init__()\n",
    "        self.use_downsample = downsample\n",
    "        if num_layer == 2 and downsample == 1:\n",
    "            self.in_channels = 16*(2**num_layer)\n",
    "        if num_layer > 2 and downsample != 0:\n",
    "            self.in_channels = 16*(2**(num_layer-1))\n",
    "        elif downsample == 0: \n",
    "            self.in_channels = 16*(2**num_layer)\n",
    "            \n",
    "        self.out_channels = 16*(2**num_layer)\n",
    "        \n",
    "        if downsample == 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "                conv1x1(self.in_channels, self.out_channels, stride=1),\n",
    "                nn.BatchNorm2d(self.out_channels))\n",
    "\n",
    "            self.conv1 = conv3x3(self.in_channels, self.out_channels,stride=2)\n",
    "        elif downsample == -1:\n",
    "            self.downsample = conv1x1(self.in_channels, self.out_channels, stride=1)\n",
    "            \n",
    "            self.conv1 = conv3x3(self.in_channels, self.out_channels, stride=1)\n",
    "            \n",
    "        elif downsample == 0:\n",
    "            self.conv1 = conv3x3(self.in_channels, self.out_channels, stride=1)\n",
    "        self.in_channels = self.out_channels\n",
    "        self.bn1 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "#         print('Block input',x.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "#         print('Block before skip',out.shape)\n",
    "\n",
    "        if self.use_downsample != 0:\n",
    "#             print('Before downsample',out.shape, skip.shape)\n",
    "#             if self.use_downsample == 1:\n",
    "#                 out = self.maxpool(out)\n",
    "            skip = self.downsample(x)\n",
    "#             print('After downsample',out.shape, skip.shape)\n",
    "        out += skip\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layer,\n",
    "        downsample = 0\n",
    "        \n",
    "    ):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        \n",
    "        self.use_downsample = downsample\n",
    "        if num_layer == 2 and downsample == 1:\n",
    "            self.in_channels = 16*(2**num_layer)\n",
    "        elif num_layer > 2 and downsample != 0:\n",
    "            self.in_channels = 16*(2**(num_layer-1))*4\n",
    "        elif downsample == 0: \n",
    "            self.in_channels = 16*(2**num_layer)*4\n",
    "            \n",
    "        self.out_channels = 16*(2**num_layer)\n",
    "   \n",
    "        if downsample == 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "                conv1x1(self.in_channels, self.out_channels*4, stride=1),\n",
    "                nn.BatchNorm2d(self.out_channels*4))\n",
    "\n",
    "            self.conv1 = conv1x1(self.in_channels, self.out_channels)\n",
    "            self.in_channels = self.out_channels\n",
    "            self.bn1 = nn.BatchNorm2d(self.out_channels)\n",
    "            self.conv2 = conv3x3(self.in_channels, self.out_channels,stride=2)\n",
    "        elif downsample == -1:\n",
    "            self.downsample = nn.Sequential(\n",
    "#                 nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "                conv1x1(self.in_channels, self.out_channels*4, stride=1),\n",
    "                nn.BatchNorm2d(self.out_channels*4))\n",
    "            \n",
    "            self.conv1 = conv1x1(self.in_channels, self.out_channels)\n",
    "            self.in_channels = self.out_channels\n",
    "            self.bn1 = nn.BatchNorm2d(self.out_channels)\n",
    "            self.conv2 = conv3x3(self.in_channels, self.out_channels)\n",
    "\n",
    "        elif downsample == 0:\n",
    "            self.conv1 = conv1x1(self.in_channels, self.out_channels)\n",
    "            self.in_channels = self.out_channels\n",
    "            self.bn1 = nn.BatchNorm2d(self.out_channels)\n",
    "            self.conv2 = conv3x3(self.in_channels, self.out_channels)\n",
    "            \n",
    "        self.bn2 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.out_channels = 16*(2**num_layer)*4\n",
    "        self.conv3 = conv1x1(self.in_channels, self.out_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.in_channels = self.out_channels\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.use_downsample != 0:\n",
    "#             print(out.shape, x.shape)\n",
    "#             if self.use_downsample == 1:\n",
    "#                 out = self.maxpool(out)\n",
    "            skip = self.downsample(x)\n",
    "#             print(out.shape, x.shape)\n",
    "        out += skip\n",
    "        out = self.relu(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс конструктор ResNet подобных архитектур\n",
    "Данный класс собирает готовую модель из ResNet-блоков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_like(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 num_classes,\n",
    "                 bottleneck,\n",
    "                 \n",
    "                 ):\n",
    "        \n",
    "        super(ResNet_like, self).__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            conv3x3(3, 32, stride=2),\n",
    "            conv3x3(32, 32, stride=2),\n",
    "            conv3x3(32, 64))\n",
    "        \n",
    "        self.body = nn.Sequential()\n",
    "        if bottleneck == True:\n",
    "            for num, layer in enumerate(layers):\n",
    "                for block in range(layer):\n",
    "                    if block == 0  and num < len(layers) - 1:\n",
    "                        downsample = 1\n",
    "                    elif block == 0 and num == len(layers) - 1:\n",
    "                        downsample = -1\n",
    "                    elif block != 0:  \n",
    "                        downsample = 0\n",
    "                    self.body.add_module(name='block_%d_%d'%(num+2,block+1), module=BottleneckBlock(num+2, downsample))\n",
    "        elif bottleneck == False:\n",
    "            for num, layer in enumerate(layers):\n",
    "                for block in range(layer):\n",
    "                    if block == 0  and num < len(layers) - 1:\n",
    "                        downsample = 1\n",
    "                    elif block == 0 and num == len(layers) - 1:\n",
    "                        downsample = -1\n",
    "                    elif block != 0:  \n",
    "                        downsample = 0\n",
    "                    self.body.add_module(name='block_%d_%d'%(num+2,block+1), module=NormalBlock(num+2, downsample))\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        if bottleneck == True:\n",
    "            self.linear_input = 32*(2**(len(layers)))*4\n",
    "        else:\n",
    "            self.linear_input = 32*(2**(len(layers)))\n",
    "        self.linear = nn.Linear(self.linear_input, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.first(x)\n",
    "#         print('Shape input body:', x.shape)\n",
    "        x = self.body(x)\n",
    "#         print('Shape input avgpool:', x.shape)\n",
    "        x = self.avgpool(x)\n",
    "#         print('Shape input linear:', x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print('Shape input linear:', x.shape)\n",
    "        x = self.linear(x)\n",
    "#         x = self.final(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчета ошибки для label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
    "        with torch.no_grad():\n",
    "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
    "                                  device=targets.device) \\\n",
    "                                  .fill_(smoothing /(n_classes-1)) \\\n",
    "                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
    "        return targets\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "        if self.reduction == 'sum' else loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "\n",
    "        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n",
    "        log_preds = torch.nn.functional.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            log_preds = log_preds * self.weight.unsqueeze(0)\n",
    "\n",
    "        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель с через конструктор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet_like(layers=resnet_layers, bottleneck=bottleneck, num_classes=num_classes)\n",
    "criterion = SmoothCrossEntropyLoss(smoothing=label_smoothing) #nn.CrossEntropyLoss()\n",
    "if optimizer_type == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "elif optimizer_type == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.95, 0.99), eps=1e-06, weight_decay=0.0001, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем диспетчер изменения скорости обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sheduler_type == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=(num_epoch-warmup_epoch)//sheduler_cycle, gamma=0.1)\n",
    "    if warmup_epoch > 0:\n",
    "        scheduler_warmup = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                         base_lr=learning_rate/(batch_per_epoch*warmup_epoch), \n",
    "                                                         max_lr=learning_rate,\n",
    "                                                         step_size_up=((batch_per_epoch+1)*warmup_epoch), # should be batch_per_epoch + 1\n",
    "                                                         step_size_down=0,\n",
    "                                                         cycle_momentum=False,\n",
    "                                                        )    \n",
    "elif sheduler_type == 'cos':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, (num_epoch-warmup_epoch)//sheduler_cycle, eta_min=0)\n",
    "    if warmup_epoch > 0:\n",
    "        scheduler_warmup = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                         base_lr=learning_rate/(batch_per_epoch*warmup_epoch), \n",
    "                                                         max_lr=learning_rate,\n",
    "                                                         step_size_up=((batch_per_epoch+1)*warmup_epoch), # should be batch_per_epoch + 1\n",
    "                                                         step_size_down=0,\n",
    "                                                         cycle_momentum=False,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ячейча используется для запуска реализации ResNet в библиотеке PyTorch для сравнения с конструктором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet34\n",
    "# model = resnet34(num_classes=10)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001, momentum=0.9)\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.95, 0.99), eps=1e-06, weight_decay=0.0001, amsgrad=False)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epoch, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модель на видеокарту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet_like(\n",
       "  (first): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (block_2_1): NormalBlock(\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_2_2): NormalBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_2_3): NormalBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_3_1): NormalBlock(\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_3_2): NormalBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_3_3): NormalBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_3_4): NormalBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_1): NormalBlock(\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_2): NormalBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_3): NormalBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_4): NormalBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_5): NormalBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_4_6): NormalBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_5_1): NormalBlock(\n",
       "      (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_5_2): NormalBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block_5_3): NormalBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем DataFrame для записи метрик обучения в процессе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./metrics/ResNet34_SGD_lr0.1_b32_cos_sc125.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_name = ['epoch', 'time', 'current_lr', 'loss', 'accuracy_train', 'accuracy_val']\n",
    "metrics_frame = pd.DataFrame(columns=cols_name)\n",
    "metrics_frame_file = (save_metrics_dir + model_name + '.csv')\n",
    "metrics_frame_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировочный цикл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Основная метрика accuracy (топ1).<br>\n",
    "- В цикле используется упрощенный подсчет accuracy в конце каждой эпохи для ускорения обучения.<br>\n",
    "Если в конце эпохи ускоренный подсчет показывает интересный результат, то метрика на тестовом датасете будет посчитана честно.\n",
    "- Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/255, Time: 45.64 sec, current_lr: 2.00e-02, Loss: 3.048, Accuracy_train: 0.252, Accuracy_val: 0.222\n",
      "Epoch 2/255, Time: 44.66 sec, current_lr: 3.99e-02, Loss: 2.318, Accuracy_train: 0.343, Accuracy_val: 0.341\n",
      "Epoch 3/255, Time: 44.74 sec, current_lr: 5.98e-02, Loss: 1.863, Accuracy_train: 0.398, Accuracy_val: 0.433\n",
      "Epoch 4/255, Time: 45.15 sec, current_lr: 7.97e-02, Loss: 2.183, Accuracy_train: 0.552, Accuracy_val: 0.539\n",
      "Epoch 5/255, Time: 45.17 sec, current_lr: 9.97e-02, Loss: 1.906, Accuracy_train: 0.497, Accuracy_val: 0.513\n",
      "Epoch 6/255, Time: 44.92 sec, current_lr: 1.00e-01, Loss: 1.552, Accuracy_train: 0.547, Accuracy_val: 0.542\n",
      "Epoch 7/255, Time: 46.28 sec, current_lr: 9.99e-02, Loss: 1.547, Accuracy_train: 0.611, Accuracy_val: 0.587\n",
      "Epoch 8/255, Time: 44.82 sec, current_lr: 9.99e-02, Loss: 1.123, Accuracy_train: 0.574, Accuracy_val: 0.618\n",
      "Epoch 9/255, Time: 44.72 sec, current_lr: 9.97e-02, Loss: 1.382, Accuracy_train: 0.618, Accuracy_val: 0.624\n",
      "Epoch 10/255, Time: 45.49 sec, current_lr: 9.96e-02, Loss: 0.814, Accuracy_train: 0.633, Accuracy_val: 0.646\n",
      "Epoch 11/255, Time: 45.16 sec, current_lr: 9.94e-02, Loss: 2.162, Accuracy_train: 0.558, Accuracy_val: 0.598\n",
      "Epoch 12/255, Time: 45.40 sec, current_lr: 9.92e-02, Loss: 1.133, Accuracy_train: 0.628, Accuracy_val: 0.663\n",
      "Epoch 13/255, Time: 44.96 sec, current_lr: 9.90e-02, Loss: 0.792, Accuracy_train: 0.707, Accuracy_val: 0.653\n",
      "Epoch 14/255, Time: 45.23 sec, current_lr: 9.87e-02, Loss: 1.266, Accuracy_train: 0.648, Accuracy_val: 0.670\n",
      "Epoch 15/255, Time: 45.48 sec, current_lr: 9.84e-02, Loss: 0.562, Accuracy_train: 0.690, Accuracy_val: 0.712\n",
      "Epoch 16/255, Time: 44.33 sec, current_lr: 9.81e-02, Loss: 1.513, Accuracy_train: 0.750, Accuracy_val: 0.732\n",
      "Epoch 17/255, Time: 44.65 sec, current_lr: 9.77e-02, Loss: 1.015, Accuracy_train: 0.768, Accuracy_val: 0.739\n",
      "Epoch 18/255, Time: 45.54 sec, current_lr: 9.74e-02, Loss: 1.107, Accuracy_train: 0.717, Accuracy_val: 0.752\n",
      "Epoch 19/255, Time: 45.28 sec, current_lr: 9.69e-02, Loss: 1.611, Accuracy_train: 0.651, Accuracy_val: 0.694\n",
      "Epoch 20/255, Time: 44.73 sec, current_lr: 9.65e-02, Loss: 0.834, Accuracy_train: 0.785, Accuracy_val: 0.764\n",
      "Epoch 21/255, Time: 45.02 sec, current_lr: 9.60e-02, Loss: 1.195, Accuracy_train: 0.721, Accuracy_val: 0.720\n",
      "Epoch 22/255, Time: 45.02 sec, current_lr: 9.55e-02, Loss: 0.584, Accuracy_train: 0.748, Accuracy_val: 0.737\n",
      "Epoch 23/255, Time: 45.06 sec, current_lr: 9.50e-02, Loss: 1.027, Accuracy_train: 0.766, Accuracy_val: 0.800\n",
      "Epoch 24/255, Time: 45.05 sec, current_lr: 9.44e-02, Loss: 1.387, Accuracy_train: 0.788, Accuracy_val: 0.772\n",
      "Epoch 25/255, Time: 45.22 sec, current_lr: 9.38e-02, Loss: 0.692, Accuracy_train: 0.758, Accuracy_val: 0.751\n",
      "Epoch 26/255, Time: 45.67 sec, current_lr: 9.32e-02, Loss: 0.979, Accuracy_train: 0.772, Accuracy_val: 0.776\n",
      "Epoch 27/255, Time: 45.51 sec, current_lr: 9.25e-02, Loss: 0.870, Accuracy_train: 0.758, Accuracy_val: 0.791\n",
      "Epoch 28/255, Time: 44.48 sec, current_lr: 9.19e-02, Loss: 1.196, Accuracy_train: 0.777, Accuracy_val: 0.761\n",
      "Epoch 29/255, Time: 44.18 sec, current_lr: 9.12e-02, Loss: 0.951, Accuracy_train: 0.792, Accuracy_val: 0.783\n",
      "Epoch 30/255, Time: 44.86 sec, current_lr: 9.05e-02, Loss: 1.117, Accuracy_train: 0.778, Accuracy_val: 0.778\n",
      "Epoch 31/255, Time: 45.14 sec, current_lr: 8.97e-02, Loss: 0.871, Accuracy_train: 0.797, Accuracy_val: 0.796\n",
      "Epoch 32/255, Time: 45.00 sec, current_lr: 8.89e-02, Loss: 1.747, Accuracy_train: 0.821, Accuracy_val: 0.768\n",
      "Epoch 33/255, Time: 45.15 sec, current_lr: 8.81e-02, Loss: 0.804, Accuracy_train: 0.790, Accuracy_val: 0.803\n",
      "Epoch 34/255, Time: 45.07 sec, current_lr: 8.73e-02, Loss: 1.163, Accuracy_train: 0.841, Accuracy_val: 0.824\n",
      "Epoch 35/255, Time: 45.06 sec, current_lr: 8.64e-02, Loss: 0.888, Accuracy_train: 0.810, Accuracy_val: 0.796\n",
      "Epoch 36/255, Time: 44.64 sec, current_lr: 8.56e-02, Loss: 0.691, Accuracy_train: 0.846, Accuracy_val: 0.794\n",
      "Epoch 37/255, Time: 44.65 sec, current_lr: 8.47e-02, Loss: 0.780, Accuracy_train: 0.821, Accuracy_val: 0.774\n",
      "Epoch 38/255, Time: 45.46 sec, current_lr: 8.38e-02, Loss: 0.685, Accuracy_train: 0.807, Accuracy_val: 0.784\n",
      "Epoch 39/255, Time: 45.23 sec, current_lr: 8.28e-02, Loss: 0.464, Accuracy_train: 0.783, Accuracy_val: 0.775\n",
      "Epoch 40/255, Time: 45.03 sec, current_lr: 8.19e-02, Loss: 1.286, Accuracy_train: 0.832, Accuracy_val: 0.821\n",
      "Epoch 41/255, Time: 45.55 sec, current_lr: 8.09e-02, Loss: 1.026, Accuracy_train: 0.792, Accuracy_val: 0.768\n",
      "Epoch 42/255, Time: 44.01 sec, current_lr: 7.99e-02, Loss: 0.606, Accuracy_train: 0.802, Accuracy_val: 0.792\n",
      "Epoch 43/255, Time: 45.45 sec, current_lr: 7.89e-02, Loss: 0.747, Accuracy_train: 0.766, Accuracy_val: 0.767\n",
      "Epoch 44/255, Time: 44.97 sec, current_lr: 7.78e-02, Loss: 0.481, Accuracy_train: 0.827, Accuracy_val: 0.780\n",
      "Epoch 45/255, Time: 44.83 sec, current_lr: 7.68e-02, Loss: 0.805, Accuracy_train: 0.817, Accuracy_val: 0.838\n",
      "Epoch 46/255, Time: 44.40 sec, current_lr: 7.57e-02, Loss: 0.390, Accuracy_train: 0.866, Accuracy_val: 0.826\n",
      "Epoch 47/255, Time: 44.81 sec, current_lr: 7.46e-02, Loss: 0.732, Accuracy_train: 0.819, Accuracy_val: 0.803\n",
      "Epoch 48/255, Time: 43.93 sec, current_lr: 7.35e-02, Loss: 0.571, Accuracy_train: 0.812, Accuracy_val: 0.813\n",
      "Epoch 49/255, Time: 44.90 sec, current_lr: 7.24e-02, Loss: 0.718, Accuracy_train: 0.851, Accuracy_val: 0.824\n",
      "Epoch 50/255, Time: 45.45 sec, current_lr: 7.13e-02, Loss: 0.459, Accuracy_train: 0.826, Accuracy_val: 0.794\n",
      "Epoch 51/255, Time: 44.56 sec, current_lr: 7.01e-02, Loss: 0.675, Accuracy_train: 0.861, Accuracy_val: 0.808\n",
      "Epoch 52/255, Time: 45.41 sec, current_lr: 6.90e-02, Loss: 0.851, Accuracy_train: 0.853, Accuracy_val: 0.832\n",
      "Epoch 53/255, Time: 43.62 sec, current_lr: 6.78e-02, Loss: 0.753, Accuracy_train: 0.882, Accuracy_val: 0.833\n",
      "Epoch 54/255, Time: 45.30 sec, current_lr: 6.66e-02, Loss: 0.534, Accuracy_train: 0.831, Accuracy_val: 0.844\n",
      "Epoch 55/255, Time: 45.06 sec, current_lr: 6.55e-02, Loss: 0.864, Accuracy_train: 0.876, Accuracy_val: 0.821\n",
      "Epoch 56/255, Time: 44.35 sec, current_lr: 6.43e-02, Loss: 0.581, Accuracy_train: 0.838, Accuracy_val: 0.798\n",
      "Epoch 57/255, Time: 44.75 sec, current_lr: 6.30e-02, Loss: 1.167, Accuracy_train: 0.871, Accuracy_val: 0.833\n",
      "Epoch 58/255, Time: 45.36 sec, current_lr: 6.18e-02, Loss: 0.824, Accuracy_train: 0.865, Accuracy_val: 0.811\n",
      "Epoch 59/255, Time: 45.12 sec, current_lr: 6.06e-02, Loss: 1.141, Accuracy_train: 0.873, Accuracy_val: 0.832\n",
      "Epoch 60/255, Time: 45.17 sec, current_lr: 5.94e-02, Loss: 1.337, Accuracy_train: 0.868, Accuracy_val: 0.811\n",
      "Epoch 61/255, Time: 45.01 sec, current_lr: 5.81e-02, Loss: 0.650, Accuracy_train: 0.860, Accuracy_val: 0.842\n",
      "Epoch 62/255, Time: 45.02 sec, current_lr: 5.69e-02, Loss: 1.183, Accuracy_train: 0.860, Accuracy_val: 0.856\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "       \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "#         print(data[1])\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#         print(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print('Loss: ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch < 5:\n",
    "            scheduler_warmup.step()\n",
    "    if epoch >= 5:\n",
    "        scheduler.step()\n",
    "    \n",
    "    #Accuracy train and val\n",
    "    model.eval()\n",
    "    correct_train, correct_val = 0, 0\n",
    "    total_train, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        trainset_subset = torch.utils.data.Subset(trainset, np.random.randint(0,high=train_img_qty, size=train_img_qty//16))\n",
    "        trainset_dataloader = torch.utils.data.DataLoader(trainset_subset, batch_size=batch_size,\n",
    "                                                            shuffle=False)\n",
    "        for images, labels in trainset_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.to(device)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        testset_subset = torch.utils.data.Subset(testset, np.random.randint(0,high=val_img_qty, size=val_img_qty//4))\n",
    "        testset_dataloader = torch.utils.data.DataLoader(testset_subset, batch_size=batch_size,\n",
    "                                                            shuffle=False)\n",
    "        for images, labels in testset_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.to(device)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "        if correct_val/total_val >= .88 and correct_val/total_val >= metrics_frame['accuracy_val'].max():\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    outputs = outputs.to(device)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "                    \n",
    "    end_time = time.time()\n",
    "    metrics = {'epoch': epoch+1,\n",
    "               'time': end_time - start_time,\n",
    "               'current_lr': [group['lr'] for group in optimizer.param_groups][0],\n",
    "               'loss': float(loss),\n",
    "               'accuracy_train': correct_train/total_train,\n",
    "               'accuracy_val': correct_val/total_val,\n",
    "               }\n",
    "\n",
    "    print(\"Epoch {}/{}, Time: {:.2f} sec, current_lr: {:.2e}, Loss: {:.3f}, Accuracy_train: {:.3f}, Accuracy_val: {:.3f}\".\n",
    "          format(metrics['epoch'], num_epoch, metrics['time'], metrics['current_lr'], metrics['loss'], metrics['accuracy_train'], metrics['accuracy_val']))\n",
    "    \n",
    "    metrics_frame = metrics_frame.append(pd.DataFrame.from_dict(metrics,orient='index').T)\n",
    "    metrics_frame.to_csv(metrics_frame_file,index=False)\n",
    "    \n",
    "    if save_best_model == True:\n",
    "        if metrics['accuracy_val'] == metrics_frame['accuracy_val'].max():\n",
    "            torch.save(model, save_model_dir + model_name + '.pt')\n",
    "            \n",
    "if save_best_model == False:\n",
    "    torch.save(model, save_model_dir + model_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка_модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем сохраненную модель на соответствие метрикам в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(save_model_dir + model_name + '.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_train, correct_val = 0, 0\n",
    "total_train, total_val = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.to(device)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_val += labels.size(0)\n",
    "        correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "# print('Accuracy final model on validation dataset is: {:.3f})'.format(correct_val/total_val))\n",
    "print(f'Accuracy final model on the validation dataset is: {(correct_val/total_val):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Критика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известные проблемы:\n",
    "- агументации сильно увеличивают время обучения (+50%)\n",
    "- прирост accuracy от использования трюков отмечен только для cosine learning decay (sheduler_type = 'cos')\n",
    "- результат очень сильно зависит от стартового learning_rate. learning_rate для используемых оптимизаторов нужно выбирать в разном диапазоне (0,01-0,1 для SGD и 0,0005-0,03 для Adam), что приводит к некоторой путаницы\n",
    "- папки './model' и './metrics' должны быть созданы самостоятельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
